{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Profiling: Gender classification on Twitter-Posts\n",
    "\n",
    "###### Name: Mohammad Zaki Gundagi\n",
    "\n",
    "Programming Language: Python 3 in Jupyter Notebook 6.0.0\n",
    "\n",
    "Python Libraries used:\n",
    "- nltk\n",
    "- pandas\n",
    "- zipfile\n",
    "- re\n",
    "- numpy\n",
    "- text2digits\n",
    "- sklearn.model_selection - train_test_split, GridSearchCV\n",
    "- sklearn.linear_model - LogisticRegression, SGDClassifier\n",
    "- sklearn.metrics - metrics\n",
    "- sklearn.svm - SVC\n",
    "- sklearn.feature_extraction.text - TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Data extraction](#sec_1)\n",
    "* [Data pre-processing and featurization](#sec_2)\n",
    "* [Classifier](#sec_3)\n",
    "* [Evaluation](#sec_4)\n",
    "* [Conclusion](#sec_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ariha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#!pip install text2digits\n",
    "from text2digits import text2digits \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# for creating logistic model and linear SVM model\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "# for creating support vector machine (SVM) model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data extraction <a class=\"anchor\" id=\"sec_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "train_label= pd.read_csv('train_labels.csv')\n",
    "test_data=pd.read_csv('test.csv')\n",
    "z= zipfile.ZipFile('data.zip','r')\n",
    "\n",
    "# creating 2 lists to save twitter posts and gender of authors for training\n",
    "train=[]\n",
    "train_gender=[]\n",
    "\n",
    "i = 0\n",
    "while i < len(train_label):\n",
    "    # encode the xml data and convert to lowercase letters\n",
    "    data=z.read('data/'+train_label['id'][i]+'.xml').decode(\"utf-8\").lower()\n",
    "\n",
    "    # extract post body\n",
    "    pattern1 = re.compile(r'cdata\\[(.*?)\\]]>',re.DOTALL)  \n",
    "    data1 = pattern1.findall(data)\n",
    "    \n",
    "    # save body data to train list\n",
    "    train = train+[data1]\n",
    "    \n",
    "    # extract gender(label)\n",
    "    train_gender = train_gender+[train_label['gender'][i]]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data pre-processing and featurization <a class=\"anchor\" id=\"sec_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove all such words with apostrophe\n",
    "contractions = {\n",
    "\"aint\": \"am not\",\n",
    "\"arent\": \"are not\",\n",
    "\"cant\": \"cannot\",\n",
    "\"cantve\": \"cannot have\",\n",
    "\"cause\": \"because\",\n",
    "\"couldve\": \"could have\",\n",
    "\"couldnt\": \"could not\",\n",
    "\"couldntve\": \"could not have\",\n",
    "\"didnt\": \"did not\",\n",
    "\"doesnt\": \"does not\",\n",
    "\"dont\": \"do not\",\n",
    "\"hadnt\": \"had not\",\n",
    "\"hadntve\": \"had not have\",\n",
    "\"hasnt\": \"has not\",\n",
    "\"havent\": \"have not\",\n",
    "\"hed\": \"he had\",\n",
    "\"hedve\": \"he would have\",\n",
    "\"hell\": \"he shall\",\n",
    "\"hellve\": \"he shall have\",\n",
    "\"hes\": \"he has\",\n",
    "\"howd\": \"how did\",\n",
    "\"howdy\": \"how do you\",\n",
    "\"howll\": \"how will\",\n",
    "\"hows\": \"how has\",\n",
    "\"id\": \"I had\",\n",
    "\"idve\": \"I would have\",\n",
    "\"ill\": \"I shall\",\n",
    "\"illve\": \"I shall have\",\n",
    "\"im\": \"I am\",\n",
    "\"ive\": \"I have\",\n",
    "\"isnt\": \"is not\",\n",
    "\"itd\": \"it had\",\n",
    "\"itdve\": \"it would have\",\n",
    "\"itll\": \"it shall\",\n",
    "\"itllve\": \"it shall have\",\n",
    "\"its\": \"it has\",\n",
    "\"lets\": \"let us\",\n",
    "\"maam\": \"madam\",\n",
    "\"maynt\": \"may not\",\n",
    "\"mightve\": \"might have\",\n",
    "\"mightnt\": \"might not\",\n",
    "\"mightntve\": \"might not have\",\n",
    "\"mustve\": \"must have\",\n",
    "\"mustnt\": \"must not\",\n",
    "\"mustntve\": \"must not have\",\n",
    "\"neednt\": \"need not\",\n",
    "\"needntve\": \"need not have\",\n",
    "\"oclock\": \"of the clock\",\n",
    "\"oughtnt\": \"ought not\",\n",
    "\"oughtntve\": \"ought not have\",\n",
    "\"shant\": \"shall not\",\n",
    "\"shant\": \"shall not\",\n",
    "\"shantve\": \"shall not have\",\n",
    "\"shed\": \"she had\",\n",
    "\"shedve\": \"she would have\",\n",
    "\"shell\": \"she shall\",\n",
    "\"shellve\": \"she shall have\",\n",
    "\"shes\": \"she has\",\n",
    "\"shouldve\": \"should have\",\n",
    "\"shouldnt\": \"should not\",\n",
    "\"shouldntve\": \"should not have\",\n",
    "\"sove\": \"so have\",\n",
    "\"sos\": \"so as\",\n",
    "\"thatd\": \"that would\",\n",
    "\"thatdve\": \"that would have\",\n",
    "\"thats\": \"that has\",\n",
    "\"thered\": \"there had\",\n",
    "\"theredve\": \"there would have\",\n",
    "\"theres\": \"there has\",\n",
    "\"theyd\": \"they had\",\n",
    "\"theydve\": \"they would have\",\n",
    "\"theyll\": \"they shall\",\n",
    "\"theyllve\": \"they shall have\",\n",
    "\"theyre\": \"they are\",\n",
    "\"theyve\": \"they have\",\n",
    "\"tove\": \"to have\",\n",
    "\"wasnt\": \"was not\",\n",
    "\"wed\": \"we had\",\n",
    "\"wedve\": \"we would have\",\n",
    "\"well\": \"we will\",\n",
    "\"wellve\": \"we will have\",\n",
    "\"were\": \"we are\",\n",
    "\"weve\": \"we have\",\n",
    "\"werent\": \"were not\",\n",
    "\"whatll\": \"what shall\",\n",
    "\"whatllve\": \"what shall have\",\n",
    "\"whatre\": \"what are\",\n",
    "\"whats\": \"what has\",\n",
    "\"whatve\": \"what have\",\n",
    "\"whens\": \"when has\",\n",
    "\"whenve\": \"when have\",\n",
    "\"whered\": \"where did\",\n",
    "\"wheres\": \"where has\",\n",
    "\"whereve\": \"where have\",\n",
    "\"wholl\": \"who shall\",\n",
    "\"whollve\": \"who shall have\",\n",
    "\"whos\": \"who has\",\n",
    "\"whove\": \"who have\",\n",
    "\"whys\": \"why has\",\n",
    "\"whyve\": \"why have\",\n",
    "\"willve\": \"will have\",\n",
    "\"wont\": \"will not\",\n",
    "\"wontve\": \"will not have\",\n",
    "\"wouldve\": \"would have\",\n",
    "\"wouldnt\": \"would not\",\n",
    "\"wouldntve\": \"would not have\",\n",
    "\"yall\": \"you all\",\n",
    "\"yalld\": \"you all would\",\n",
    "\"yalldve\": \"you all would have\",\n",
    "\"yallre\": \"you all are\",\n",
    "\"yallve\": \"you all have\",\n",
    "\"youd\": \"you had\",\n",
    "\"youdve\": \"you would have\",\n",
    "\"youll\": \"you shall\",\n",
    "\"youllve\": \"you shall have\",\n",
    "\"youre\": \"you are\",\n",
    "\"youve\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tokenization and removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "i = 0\n",
    "while i < len(train):\n",
    "    data=train[i]\n",
    "    word_set=[]\n",
    "    for sentences in data:\n",
    "        \n",
    "        # remove space>1\n",
    "        sentences= re.sub(r'\\s+', ' ', sentences)\n",
    "        \n",
    "        # remove apostrophe s\n",
    "        sentences= re.sub(r'\\'s|\\' s', '', sentences)\n",
    "        \n",
    "        # remove symbols\n",
    "        sentences= re.sub(r'[^\\w\\s]','', sentences)\n",
    "        \n",
    "        # remove English language contractions\n",
    "        for word in sentences.split():\n",
    "            if word in contractions:\n",
    "                sentences = sentences.replace(word, contractions[word])\n",
    "                \n",
    "        # split by space \n",
    "        word = sentences.split(' ')  \n",
    "        word1=[]\n",
    "        \n",
    "        for string in word:\n",
    "            # delete web link\n",
    "            w=re.sub(r'https.*', '', string)\n",
    "            word1=word1+[w]\n",
    "            \n",
    "        word_set=word_set+word1\n",
    "    clean = []\n",
    "    # remove stop words\n",
    "    filtered_words = [word for word in word_set if word not in stopwords.words('english')]\n",
    "    for word in filtered_words:\n",
    "        if len(word)>0:\n",
    "            clean = clean+[word]\n",
    "    words=words+[clean]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Removing the most/least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "dis_words=[]\n",
    "while i < len(words):\n",
    "    data = words[i]\n",
    "    # distinct words in each person's post\n",
    "    dis_words=dis_words+[list(set(data))]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for lists in dis_words:\n",
    "    for item in lists:\n",
    "        # library to count word frequency\n",
    "        word_dict[item] = word_dict.get(item,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list=[]\n",
    "for key in word_dict:\n",
    "    # most and least frequent words\n",
    "    if word_dict[key]>3100*0.95 or word_dict[key]< 3100*0.05:\n",
    "        remove_list = remove_list+[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word=[]\n",
    "i = 0\n",
    "while i < len(words):\n",
    "    data= words[i]\n",
    "    clean = []\n",
    "    for word in data:\n",
    "        if word not in remove_list:\n",
    "            clean = clean+[word]\n",
    "    clean_word = clean_word+[clean]\n",
    "    i = i+1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data.npy',clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_data = np.load('data.npy',allow_pickle=True)\n",
    "preprocessed_train_data = preprocessed_train_data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 Removing numbers and small words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d = text2digits.Text2Digits() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numerical numbers to digits\n",
    "preprocessed_train_data = [[t2d.convert(word) for word in arr] for arr in preprocessed_train_data]\n",
    "\n",
    "# remove numbers\n",
    "preprocessed_train_data=[[word for word in arr if word.isalpha()] for arr in preprocessed_train_data]\n",
    "\n",
    "# remove single and 2 letter words\n",
    "preprocessed_train_data = [[word for word in arr if len(word) >= 3] for arr in preprocessed_train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "preprocessed_train_data = [[stemmer.stem(word) for word in arr] for arr in preprocessed_train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Create dataframe to be used in featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data1 = list(zip(preprocessed_train_data, train_gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(review_data1, columns =['Words', 'gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 TF-IDF feature extraction for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)\n",
    "x_train = vectorizer.fit_transform(train_df['Words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding featurization technique - `Not being used for the optimal model`\n",
    "\n",
    "The following code is just for representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def one_hot():  \n",
    "    input = preprocessed_train_data</t>\n",
    "    dis_list =[]  \n",
    "    for word in input:  \n",
    "        dis_list = dis_list+word  \n",
    "        dis_list = list(set(dis_list))`<br>\n",
    "        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`# create an empty dataframe with unique words\n",
    "        df=pd.DataFrame(columns = dis_list)\n",
    "    z = 0\n",
    "    while z < len(input):  \n",
    "        df.loc[z] = 0\n",
    "        for item in input[z]:`<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`# add one for each appear\n",
    "            df.loc[z,item] = df.loc[z,item]+1 \n",
    "        z =z+1  \n",
    "    return df  \n",
    "df=one_hot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classifier <a class=\"anchor\" id=\"sec_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def test_accuracy():  \n",
    "    train_error = []  \n",
    "    test_error = []  \n",
    "    i=1\n",
    "    while i < 100:  \n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)  \n",
    "        neigh.fit(df, train_gender)  \n",
    "        predict = neigh.predict(df)  \n",
    "        predict1 = neigh.predict(test_df)  \n",
    "        z = 0  \n",
    "        a = 0  \n",
    "        b = 0  \n",
    "        while z < len(predict):  \n",
    "            if predict[z]!=train_gender[z]:  \n",
    "                a=a+1  \n",
    "            z=z+1  \n",
    "        z = 0  \n",
    "        while z < len(predict1):  \n",
    "            if predict1[z]!=test_gender[z]:  \n",
    "                b=b+1  \n",
    "            z = z+1  \n",
    "        train_error = train_error+[a]  \n",
    "        test_error = test_error+[b]  \n",
    "        i = i+1\n",
    "    for item in test_error:  \n",
    "        print((500-item)/500)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Model Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the dataset into traning data(90%) and validation data/test data(10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train, train_df['gender'],test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the dolver as 'liblinear', we support both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(LogisticRegression(random_state=1, solver='liblinear'),param_grid).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy for 90% train data and 10% test data(from train data only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score( y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divinding the training data into 80-20 ratio. 80 - training and 20 - testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Doing 80-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train, train_label['gender'], test_size=0.2, random_state=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying a simple SVM model and testing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Building SVM model\n",
    "clf_1 = SVC(probability=True, kernel='rbf')\n",
    "clf_1.fit(X_train, y_train)`<br>\n",
    "\n",
    "`# Making predictions and testing accuracy\n",
    "predictions = clf_1.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test,predictions))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7758064516129032\n",
    "\n",
    "#### Trying another model with modified parameters and testing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Building SVM model\n",
    "clf_2 = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "clf_2.fit(X_train, y_train)`<br>\n",
    "\n",
    "`# Making predictions and testing accuracy\n",
    "predictions = clf_2.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test,predictions))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7725806451612903\n",
    "\n",
    "We notice simpler model worked better than this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find the best model using Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Parameters\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}`<br>\n",
    "\n",
    "`# Building several SVM models\n",
    "gs_clf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(gs_clf.best_params_)`<br>\n",
    "\n",
    "`# Utilising the best model chosen\n",
    "predictions = gs_clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test,predictions))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'C': 1, 'gamma': 1} <br>\n",
    "0.7774193548387097\n",
    "\n",
    "The best model selected has an accuracy of 77.74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation <a class=\"anchor\" id=\"sec_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Reading and preprocessing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list to save twitter posts of authors for evaluation\n",
    "test_body = []\n",
    "\n",
    "i = 0\n",
    "while i < len(test_data):\n",
    "    # encode the xml data and convert to lowercase letters\n",
    "    data=z.read('data/'+test_data['id'][i]+'.xml').decode(\"utf-8\").lower()\n",
    "    \n",
    "    # extract post body\n",
    "    pattern = re.compile(r'cdata\\[(.*?)\\]]>',re.DOTALL)  \n",
    "    data1 = pattern.findall(data)\n",
    "    \n",
    "    # saving the posts in the list\n",
    "    test_body = test_body+[data1]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Tokenization and removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test=[]\n",
    "i = 0\n",
    "while i < len(test_body):\n",
    "    data=test_body[i]\n",
    "    word_set=[]\n",
    "    for sentences in data:\n",
    "        # remove space>1\n",
    "        sentences= re.sub(r'\\s+', ' ', sentences)\n",
    "        \n",
    "        # remove apostrophe s\n",
    "        sentences= re.sub(r'\\'s|\\' s', '', sentences)\n",
    "        \n",
    "        # remove symbols\n",
    "        sentences= re.sub(r'[^\\w\\s]','',sentences)\n",
    "        \n",
    "        # remove English language contractions\n",
    "        for word in sentences.split():\n",
    "            if word in contractions:\n",
    "                sentences = sentences.replace(word, contractions[word])\n",
    "                \n",
    "        # split by space\n",
    "        word = sentences.split(' ')\n",
    "        word1=[]\n",
    "        \n",
    "        for string in word:\n",
    "            # delete web link\n",
    "            w=re.sub(r'https.*', '', string)\n",
    "            word1=word1+[w]\n",
    "        \n",
    "        word_set=word_set+word1\n",
    "        \n",
    "    clean = []\n",
    "    # remove stop words\n",
    "    filtered_words = [word for word in word_set if word not in stopwords.words('english')]\n",
    "    for word in filtered_words:\n",
    "        if len(word)>0:\n",
    "            clean = clean+[word]\n",
    "            \n",
    "    words_test=words_test+[clean]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.3 Removing the most/least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "dis_words=[]\n",
    "while i < len(words_test):\n",
    "    data = words_test[i]\n",
    "    # distinct words in each person's post\n",
    "    dis_words=dis_words+[list(set(data))]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "for lists in dis_words:\n",
    "    for item in lists:\n",
    "        # library to count word frequency\n",
    "        word_dict[item] = word_dict.get(item,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list=[]\n",
    "for key in word_dict:\n",
    "    # most and least frequent words\n",
    "    if word_dict[key]>500*0.95 or word_dict[key]< 500*0.05:\n",
    "        remove_list = remove_list+[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word_test=[]\n",
    "i = 0\n",
    "while i < len(words_test):\n",
    "    data= words_test[i]\n",
    "    clean = []\n",
    "    for word in data:\n",
    "        if word not in remove_list:\n",
    "            clean = clean+[word]\n",
    "    clean_word_test = clean_word_test+[clean]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved preprocessed data for quick future use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data_test.npy',clean_word_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_data=np.load('data_test.npy',allow_pickle=True)\n",
    "preprocessed_test_data=preprocessed_test_data.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Removing numbers and small words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numerical numbers to digits\n",
    "preprocessed_test_data = [[t2d.convert(word) for word in arr] for arr in preprocessed_test_data]\n",
    "\n",
    "# remove numbers\n",
    "preprocessed_test_data=[[word for word in arr if word.isalpha()] for arr in preprocessed_test_data]\n",
    "\n",
    "# remove single and 2 letter words\n",
    "preprocessed_test_data = [[word for word in arr if len(word) >= 3] for arr in preprocessed_test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "preprocessed_test_data = [[stemmer.stem(word) for word in arr] for arr in preprocessed_test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 TF-IDF feature extraction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test =vectorizer.transform(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1 Training Logistic Model over complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(LogisticRegression(random_state=1, solver='liblinear'),param_grid).fit(x_train,train_df['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 Predicting the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test=clf.predict(real_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 Writing Labels Into CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1 = pd.DataFrame( columns =['id', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1['gender']=pred_test\n",
    "test_data1['id']=test_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1.to_csv(\"pred_labels.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.1 create the dataframe with the same header of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i = 0  \n",
    "dis_list =[]  \n",
    "while i < len(preprocessed_train_data):`<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`# unique words\n",
    "    data = list(set(preprocessed_train_data[i]))\n",
    "    dis_list = dis_list + data\n",
    "    i=i+1\n",
    "dis_list = list(set(dis_list))`<br>\n",
    "`# create an empty dataframe with unique words\n",
    "test_df=pd.DataFrame(columns = dis_list)\n",
    "z = 0  \n",
    "while z < len(clean_test):\n",
    "    test_df.loc[z] = 0\n",
    "    for item in preprocessed_test_data[z]:\n",
    "        if item in dis_list:`<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`# add one for each appear\n",
    "            test_df.loc[z,item] = test_df.loc[z,item]+1\n",
    "    z =z+1\n",
    "test_df.head()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8.2 check the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_gender = pd.read_csv('test_labels.csv')['gender']  \n",
    "test_accuracy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1 Using the best params found to create the model\n",
    "C = 1.0 and gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Building SVM model\n",
    "clf = SVC(probability=True, C=1.0, kernel='rbf', gamma=1)\n",
    "clf.fit(X_train, y_train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2 Making predictions and finding accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Making predictions and finding accuracy\n",
    "predictions = clf.predict(real_test)\n",
    "labelled_test_data = pd.read_csv('test_labels.csv')\n",
    "print(metrics.accuracy_score(labelled_test_data['gender'],predictions))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the code above, we find that the SVM model performed well and has an accuracy of 77.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions <a class=\"anchor\" id=\"sec_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the logistic regression model (best performer) for test data is 78.66%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
